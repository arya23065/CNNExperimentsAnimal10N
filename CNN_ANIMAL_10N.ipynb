{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# ***Animal Identification using Convolutional Neural Networks***\n",
        "\n",
        "Arya Subramanyam\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UseHLcoRIYz"
      },
      "source": [
        "## **Install Deep Lake**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "l5mOffq5RN-T",
        "outputId": "aae33ee1-603a-49a0-d45d-c86c0324e2af"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip3 install deeplake\n",
        "!pip install torchnet\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGo53ndMTCB"
      },
      "source": [
        "## **Data Preprocessing**\n",
        "\n",
        "Reference = https://docs.activeloop.ai/tutorials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neD2jhKDQ5WD",
        "outputId": "87be1c95-4e19-4b46-c10a-b4a376ac0063"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, time\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Connect to the training and testing datasets\n",
        "ds_train = deeplake.load('hub://activeloop/animal10n-train')\n",
        "ds_test = deeplake.load('hub://activeloop/animal10n-test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np5fIbViHlCu"
      },
      "source": [
        "Transform the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqdWgumwQ1d6"
      },
      "outputs": [],
      "source": [
        "tform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # using ImageNet mean and std\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGmWr44PIQMk"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeiU4LobROdE"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Since torchvision transforms expect PIL images, we use the 'pil' decode_method for the 'images' tensor. This is much faster than running ToPILImage inside the transform\n",
        "train_loader = ds_train.pytorch(num_workers = 0, shuffle = True, transform = {'images': tform, 'labels': None}, batch_size = batch_size, decode_method = {'images': 'pil'})\n",
        "test_loader = ds_test.pytorch(num_workers = 0, transform = {'images': tform, 'labels': None}, batch_size = batch_size, decode_method = {'images': 'pil'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma1cTn02Trjd",
        "outputId": "f0c344dd-a2c6-41af-adf8-281a2d74a966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLXbDlWp_o-K"
      },
      "source": [
        "## **Choose which model to train**\n",
        "\n",
        "Uncomment the code of the model you wish to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZysIQEyik08"
      },
      "source": [
        "### Baseline CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snrfKAbA_sCw"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class BaselineCNN(nn.Module):\n",
        "    \n",
        "#     def __init__(self):\n",
        "#         super(BaselineCNN, self).__init__()\n",
        "        \n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(32)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(32)\n",
        "#         self.relu2 = nn.ReLU()\n",
        "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "#         self.fc1 = nn.Linear(in_features=32 * 14 * 14, out_features=228)\n",
        "#         self.fc2 = nn.Linear(in_features=228, out_features=10)\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.bn1(x)\n",
        "#         x = self.relu1(x)\n",
        "#         x = self.maxpool1(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.bn2(x)\n",
        "#         x = self.relu2(x)\n",
        "#         x = self.maxpool2(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# model = BaselineCNN()\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA8ELsfwiJu0"
      },
      "source": [
        "### ResNet from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6Xq4Ui3iKPn"
      },
      "outputs": [],
      "source": [
        "# model = models.resnet18(pretrained=False)\n",
        "# model.fc = torch.nn.Linear(model.fc.in_features, len(ds_train.labels.info.class_names))\n",
        "# model.to(device)\n",
        "\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snt5b6qwIZQ_"
      },
      "source": [
        "### ResNet Pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRBRaROLROUf",
        "outputId": "db491df7-fcae-48d1-edbb-4df8c0a61fcf"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(ds_train.labels.info.class_names))\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sVS5lTFI-gZ"
      },
      "source": [
        "## **Training the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V65Xr8aBJCUL"
      },
      "source": [
        "Reference = https://docs.activeloop.ai/tutorials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6cQJkHeJGtk"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for i, data in enumerate(data_loader):\n",
        "        inputs = data['images']\n",
        "        labels = torch.squeeze(data['labels'])\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "    \n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 0:    # print every 100 batches\n",
        "            batch_time = time.time()\n",
        "            speed = (i+1)/(batch_time-start_time)\n",
        "            print('[%5d] loss: %.3f, speed: %.2f, accuracy: %.2f %%' %\n",
        "                  (i, running_loss, speed, accuracy))\n",
        "\n",
        "            running_loss = 0.0\n",
        "            total = 0\n",
        "            correct = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kFOoGaWq14m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def test_model(model, data_loader, device, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    start_time = time.time()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_loader):\n",
        "            inputs = data['images']\n",
        "            labels = torch.squeeze(data['labels'])\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs.float())\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            true_positives += ((predicted == labels) & (labels == 1)).sum().item()\n",
        "            false_positives += ((predicted != labels) & (labels == 0)).sum().item()\n",
        "            false_negatives += ((predicted != labels) & (labels == 1)).sum().item()\n",
        "\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "        f1_score = 2 * precision * recall / (precision + recall + 1e-7)\n",
        "\n",
        "        print('Finished Testing')\n",
        "        print('Epoch: %d' % (epoch))\n",
        "        print('Testing accuracy: %.1f %%' % (accuracy))\n",
        "        print('Precision: %.4f' % (precision))\n",
        "        print('Recall: %.4f' % (recall))\n",
        "        print('F1 Score: %.4f' % (f1_score))\n",
        "\n",
        "        class_names = ds_test.labels.info.class_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWzFjzLJINu"
      },
      "source": [
        "**Train and test for 20 epochs!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fMhm4VjDRf7i",
        "outputId": "059b57d4-9ccd-449f-bcc1-a13cc61f6e41"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    print(\"------------------ Training Epoch {} ------------------\".format(epoch+1))\n",
        "    train_one_epoch(model, optimizer, train_loader, device)\n",
        "\n",
        "    test_model(model, test_loader, device, epoch)\n",
        "  \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYrrI5FTKpBe"
      },
      "outputs": [],
      "source": [
        "save_model = model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2xSFak69ixC"
      },
      "outputs": [],
      "source": [
        "model = save_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLrlDA2En-3V"
      },
      "source": [
        "## **Experiments and Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH27-nt_oNou"
      },
      "source": [
        "### Dataset visualization image grid\n",
        "\n",
        "Reference - https://leslietj.github.io/2020/07/15/PyTorch-Implementation-of-Class-Activation-Map-CAM/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vNo0QtFeYuM"
      },
      "outputs": [],
      "source": [
        "# denormalize and show an image \n",
        "def imshow(image, title=None):\n",
        "    image = image.numpy().transpose((1, 2, 0)) \n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = image * std + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "ZPEm2VDbfbHe",
        "outputId": "13f5383c-cc1d-423a-cc14-69ea8042ffee"
      },
      "outputs": [],
      "source": [
        "import torchvision \n",
        "\n",
        "# get a batch of training data\n",
        "images, classes = next(iter(test_loader))\n",
        "\n",
        "# make a grid from batch\n",
        "images = torchvision.utils.make_grid(images)\n",
        "\n",
        "class_names = ds_test.labels.info.class_names\n",
        "imshow(images, title=[class_names[x] for x in classes])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64q1MXLhoL7V"
      },
      "source": [
        "### Saliency Map\n",
        "\n",
        "Reference = ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "StNjyG7R0gfe",
        "outputId": "1fb32406-afb5-475e-b2b1-fd6a95049dab"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_saliency_map(model, image, label, device):\n",
        "    model.eval()\n",
        "\n",
        "    image = image.to(device)\n",
        "    image.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image.unsqueeze(0))\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = torch.nn.functional.cross_entropy(output, torch.tensor([label]).to(device))\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Get the gradients of the input image\n",
        "    gradients = torch.abs(image.grad)\n",
        "    \n",
        "    # Compute the saliency map by taking the maximum gradient across channels\n",
        "    saliency_map, _ = torch.max(gradients, dim=0)\n",
        "    \n",
        "    # Normalize the saliency map\n",
        "    saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-7)\n",
        "\n",
        "    saliency_map = saliency_map.cpu().numpy()\n",
        "\n",
        "    return saliency_map\n",
        "\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "  image = data['images'][0]\n",
        "  label = data['labels'][0]\n",
        "  break\n",
        "\n",
        "saliency_map = compute_saliency_map(model, image, label, device)\n",
        "\n",
        "# Denormalize\n",
        "image = image.numpy().transpose((1, 2, 0)) \n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "image = image * std + mean\n",
        "image = np.clip(image, 0, 1)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(f'Image with Label: Hamster')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(saliency_map, cmap='hot')\n",
        "plt.colorbar()\n",
        "plt.title('Saliency Map')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsxb7hREoP1U"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "FD65lZrTByau",
        "outputId": "83486a80-2863-4cca-8afc-155e8276f3c1"
      },
      "outputs": [],
      "source": [
        "from torchnet.meter import ConfusionMeter\n",
        "\n",
        "model.eval()\n",
        "\n",
        "num_classes = 10\n",
        "confusion_meter = ConfusionMeter(num_classes)\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    inputs = data['images']\n",
        "    labels = torch.squeeze(data['labels'])\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(inputs.float())\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    confusion_meter.add(predicted, labels)\n",
        "\n",
        "confusion_matrix = confusion_meter.conf\n",
        "\n",
        "plt.imshow(confusion_matrix, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNgQeW5-oTP8"
      },
      "source": [
        "### Model predictions visualizer\n",
        "\n",
        "Reference = https://leslietj.github.io/2020/07/15/PyTorch-Implementation-of-Class-Activation-Map-CAM/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "zCMSiUeDda4X",
        "outputId": "c79b4466-b38b-4e59-c461-039b09766679"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generic function to display predictions for a few images\n",
        "def visualize_model(model, test_loader, class_names, num_images=6, device='cpu'):\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, data in enumerate(test_loader):\n",
        "            if step < 0:\n",
        "              continue \n",
        "\n",
        "            images = data['images']\n",
        "            labels = torch.squeeze(data['labels'])\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs.float())\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            for i in range(images.size(0)):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[i].item()]))\n",
        "                imshow(images.cpu().data[i])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    return\n",
        "\n",
        "\n",
        "visualize_model(model, test_loader, ds_test.labels.info.class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M5rcMLp_F_q"
      },
      "source": [
        "### Accuracy Plots\n",
        "\n",
        "The accuracy values have been manually added from the epochs log for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "08eWX08x_H77",
        "outputId": "a91cec2b-5248-4539-cfcf-eeadf0d9bcf4"
      },
      "outputs": [],
      "source": [
        "# Baseline \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
        "training_accuracy = [38.09, 44.50, 49.3, 47.62, 48.50, 49.69, 49.53, 50.19, 52.34, 53.84, 53.12, 53.66, 54.09, 53.44, 55.62, 55.22, 56.00, 55.97, 55.91, 56.09 ]\n",
        "\n",
        "plt.plot(epochs, training_accuracy, 'bo-', label=\"Training accuracy\")\n",
        "plt.title(\"Baseline CNN\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(epochs) \n",
        "plt.ylim(20, 100)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Resnet from Scratch \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
        "training_accuracy = [35.44, 40.41, 44.06, 46.66, 45.88, 49.69, 47.84, 50.56, 51.69, 52.50, 52.25, 53.56, 55.62, 54.22, 54.91, 56.53, 57.97, 56.62, 59.03, 58.16]\n",
        "\n",
        "plt.plot(epochs, training_accuracy, 'bo-', label=\"Training accuracy\")\n",
        "plt.title(\"ResNet-18 from Scratch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(epochs) \n",
        "plt.ylim(20, 100)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Resnet pre-trained\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
        "training_accuracy = [68.78, 72.38, 76.19, 77.84, 77.25, 77.72, 79.53, 80.41, 80.38, 81.31, 82.34, 81.72, 83.06, 84.72, 84.47, 83.75, 85.78, 86.22, 87.22, 87.62]\n",
        "    \n",
        "plt.plot(epochs, training_accuracy, 'bo-', label=\"Training accuracy\")\n",
        "plt.title(\"ResNet-18 Pretrained\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(epochs) \n",
        "plt.ylim(20, 100)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
